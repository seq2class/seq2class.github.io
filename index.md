# 601.765 Machine Learning: Linguistic & Sequence Modeling

* **Lectures:** 3-3:50pm MWF, in Hackerman 320.
  * Sometimes we may have to do 3-4:15 but this will be announced in advance.
* **Instructors:** Jason Eisner, Ryan Cotterell
  * **Office hours:** 4-4:30pm after class, or by appointment.
* **TA:** Matthew Francis-Landau
  * **Office hours:** TBA
* **Discussion site:** [https://piazza.com/class/jd0fhovutom2kf](https://piazza.com/class/jd0fhovutom2kf)
* **Email:** cs765-staff at cs.jhu.edu
* **Class notes:**
** [Formalisms and terminology](https://github.com/seq2class/scribe-notes/blob/master/formalisms.pdf)
** [Scribe Notes](https://seq2class.github.io/scribe-notes/)
*** [LaTeX repo](https://github.com/seq2class/scribe-notes)
* **Video recordings:** TBA

## Course description
This course surveys formal ingredients that are used to build structured models of character and word sequences. We will unpack recent deep learning architectures that consider various kinds of latent structure, and see how they draw on earlier work in structured prediction, dimensionality reduction, Bayesian nonparametrics, multi-task learning, etc. We will also examine a range of strategies used for inference and learning in these models. Students will be expected to read recent papers and carry out a research project. [Applications or Analysis]

Prerequisites: EN.600/601.465/665 or permission. Prior coursework in statistics or machine learning is recommended. Students may wish to prepare for their choice of research project by taking EN.601.382 Deep Learning Lab at the same time.

## Topics list
1. Introduction
2. Sequence labeling as a canonical problem
3. Statistical background (emphasize that this part is more general than sequence modeling)
4. Algorithmic background: Paths in graphs
5. Basic non-neural sequence labeling models
6. Moving up to very large alphabets and/or state spaces - approximation algorithms
7. Feature / Architecture Engineering
8. Neuralization
9. Word embeddings (non-task-specific)
10. Optimization methods
11. Model selection
12. Deep generative models
13. Distributions over other kinds of discrete structures
14. Transition systems for parsing
15. Integration over hidden variables
16. Reinforcement learning
17. Continuous generalizations
18. Exchangeability
19. Types vs. tokens (hierarchical)
20. Lambek calculus / CCG / automata / other models of grammaticality
21. Spectral learning
22. Structure learning

## Requirements
